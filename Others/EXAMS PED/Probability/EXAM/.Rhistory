z1
z2
# Chunk 21
zL1 = -0.6745-1.5*1.349
zU1 = 0.6745+1.5*1.349
zL1
zU1
# Chunk 22
ProbL1 = pdist("norm", q = zL1, mean = 0, sd = 1, plot = FALSE)
ProbU1 = pdist("norm", q = zU1, mean = 0, sd = 1, plot = FALSE)
ProbIN = ProbU1-ProbL1
ProbOUT = 1-ProbIN
ProbOUT
View(Chile)
View(Chile)
# Chunk 1
library(mosaic)
# Chunk 2
reading <- read.table("http://asta.math.aau.dk/dan/static/datasets?file=reading.dat", header=TRUE)
head(reading)
# Chunk 3
bwplot(Response ~ Treatment , data = reading)
# Chunk 4
favstats(Response ~ Treatment, data = reading)
# Chunk 5
Control <- subset(reading, Treatment == "Control")
Treated <- subset(reading, Treatment == "Treated")
y1 = mean(~ Response, data = Control)
y2 = mean(~ Response, data = Treated)
y2
# Chunk 6
n1 = 23
n2 = 21
s1 = sd(~ Response, data = Control)
s2 = sd(~ Response, data = Treated)
s2
# Chunk 7
se2 = s2/sqrt(n2)
se2
# Chunk 8
t<-qdist("t", p = 0.025, df = 21-1, plot = FALSE)
y2+c(t*se2,-t*se2)
# Chunk 9
t.test( ~Response, data = Treated, conf.level = 0.95)
# Chunk 10
t.test(Response ~ Treatment , data = reading)
# Chunk 11
sed = sqrt(s1^2/n1+s2^2/n2)
sed
# Chunk 12
t = (y1-y2)/sed
t
# Chunk 13
P = 2*pdist("t", q = t, df = 37.855)
P
# Chunk 14
t2 <- qdist("t", p = 0.025, df = 37.855)
t2
# Chunk 15
d = y1-y2
d+c(t2*sed,-t2*sed)
# Chunk 16
M = 0.06
pi = 0.7
z<-qdist("norm", p= 0.05, mean = 0, sd = 1, plot = FALSE)
n = pi*(1-pi)*(z/M)^2
n
# Chunk 17
M = 0.06
pi = 0.5
z<-qdist("norm", p= 0.05, mean = 0, sd = 1, plot = FALSE)
n = pi*(1-pi)*(z/M)^2
n
View(Treated)
View(Treated)
View(Control)
View(Control)
View(reading)
View(reading)
# Chunk 1
library(mosaic)
# Chunk 2
reading <- read.table("http://asta.math.aau.dk/dan/static/datasets?file=reading.dat", header=TRUE)
head(reading)
# Chunk 3
bwplot(Response ~ Treatment , data = reading)
# Chunk 4
favstats(Response ~ Treatment, data = reading)
# Chunk 5
Control <- subset(reading, Treatment == "Control")
Treated <- subset(reading, Treatment == "Treated")
y1 = mean(~ Response, data = Control)
y2 = mean(~ Response, data = Treated)
y2
# Chunk 6
n1 = 23
n2 = 21
s1 = sd(~ Response, data = Control)
s2 = sd(~ Response, data = Treated)
s1
s2
# Chunk 7
se2 = s2/sqrt(n2)
se2
# Chunk 8
t<-qdist("t", p = 0.025, df = 21-1)
t
y2+c(t*se2,-t*se2)
# Chunk 9
t.test( ~Response, data = Treated, conf.level = 0.95)
# Chunk 10
t.test(Response ~ Treatment , data = reading)
# Chunk 11
sed = sqrt(s1^2/n1+s2^2/n2)
sed
# Chunk 12
t = (y1-y2)/sed
t
# Chunk 13
P = 2*pdist("t", q = t, df = 37.855)
P
# Chunk 14
t2 <- qdist("t", p = 0.025, df = 37.855)
t2
# Chunk 15
d = y1-y2
d+c(t2*sed,-t2*sed)
# Chunk 16
M <- 0.06
pi <- 0.7
z<-qdist("norm", p= 0.05, mean = 0, sd = 1) z
n <- pi*(1-pi)*(z/M)^2
n
# Chunk 17
M = 0.06
pi = 0.5
z<-qdist("norm", p= 0.05, mean = 0, sd = 1, plot = FALSE)
n = pi*(1-pi)*(z/M)^2
n
library(mosaic)
# Chunk 1
library(mosaic)
# Chunk 2
HousePrices <- read.table("http://asta.math.aau.dk/dan/static/datasets?file=HousePrice.dat", header=TRUE)
head(HousePrices)
# Chunk 3
splom(HousePrices, pch = 16, cex = 0.5)
cor(HousePrices)
# Chunk 4
cor.test(~Taxes+Size, data=HousePrices)
# Chunk 5
model <- lm( Price ~ Taxes+Size, data = HousePrices )
model
# Chunk 6
summary(model)
# Chunk 7
rSquared <- summary(model)$r.squared
y_hat <- predict(model)
lm(y_hat ~ HousePrices$Price)
xyplot(y_hat ~ HousePrices$Price, type = c("p", "r"), main = paste("Correlation between predicted and observed y ( r =", round(sqrt(rSquared),2), ")"))
# Chunk 8
7.722e-1*(sd(HousePrices$Price)/sd(y_hat))
cor(HousePrices$Price,y_hat)
# Chunk 9
confint(model)
# Chunk 10
1 - pdist("f", 164.4, df1=2, df2=97, plot = FALSE)
# Chunk 11
model2 <- lm( Price ~ Taxes*Size, data = HousePrices )
summary(model2)
View(HousePrices)
View(HousePrices)
splom(HousePrices,pch=16,cex=0.5)
'prof: so here you can see size y x taxes, very gh corrreltaion so , intrersting the covariant linearly independent'
'how to read the graph? each of the rows variable is in the y axes. so for example the first square 1left upper part, the size is Y and X is Taxes.'
'we can see that the 3 variables are highly related to each other in a sense that they make a more or less a linear correlation. AS one of the variables increase the other one also, so we expect to have high values of correlation between variables'
View(HousePrices)
View(HousePrices)
P = 2*pdist("t", q = -2.116, df = 97)
P
summary(model)
e<-min(HousePrices)
e
summary(model)
e<-min(model)
e
summary(model)
e<-min(model)
e
summary(model)
e<-min(model)
e
summary(model)
bwplot(price , data = HousePrices)
rSquared <- summary(model)$r.squared
y_hat <- predict(model)
lm(y_hat ~ HousePrices$Price)
xyplot(y_hat ~ HousePrices$Price, type = c("p", "r"), main = paste("Correlation between predicted and observed y ( r =", round(sqrt(rSquared),2), ")"))
rSquared <- summary(model)$r.squared
y_hat <- predict(model)
y_hat
lm(y_hat ~ HousePrices$Price)
xyplot(y_hat ~ HousePrices$Price, type = c("p", "r"), main = paste("Correlation between predicted and observed y ( r =", round(sqrt(rSquared),2), ")"))
rSquared <- summary(model)$r.squared
y_hat <- predict(model)
y_hat
lm(y_hat ~ HousePrices$Price)
xyplot(y_hat ~ HousePrices$Price, type = c("p", "r"), main = paste("Correlation between predicted and observed y ( r =", round(sqrt(rSquared),2), ")"))
t<-qdist("t", p = 0.025, df = 100-3)
t
# Chunk 1
vitcap <- read.table("http://asta.math.aau.dk/dan/static/datasets?file=vitcap.dat", header=TRUE)
head(vitcap)
# Chunk 2
library(mosaic)
# Chunk 3
boxplot(vital.capacity~exposure, data = vitcap)
abline(h=mean(vitcap$vital.capacity), col = "red", lwd = 2)
xyplot(vital.capacity~exposure, data = vitcap)
# Chunk 4
names(vitcap)
model_exposure <- lm(vital.capacity~exposure, data = vitcap)
model_exposure
# Chunk 5
summary(model_exposure)
# Chunk 6
xyplot(vital.capacity~age, groups = exposure, data = vitcap, type = c("p","r"), auto.key = TRUE)
# Chunk 7
model_main<-lm(vital.capacity~exposure+age,data = vitcap)
model_main
summary(model_main)
# Chunk 8
model_age<-lm(vital.capacity~age,data = vitcap)
summary(model_age)
# Chunk 9
model_interaction<-lm(vital.capacity~age*exposure,data = vitcap)
summary(model_interaction)
# Chunk 10
anova(model_exposure, model_main)
anova(model_main,model_interaction)
# Chunk 11
model1 <- lm(vital.capacity ~ age*z2, data = vitcap)
model2 <- lm(vital.capacity ~ age*z1 + age*z2, data = vitcap)
# Chunk 13
anova(model1,model2)
View(vitcap)
# Chunk 1: setup
knitr::opts_chunk$set(eval = TRUE)
# Chunk 2
CO2_dataset<-co2
'why 1959, because the data starts in that year, why freq=12? 12 months a year'
plot(co2)
time_series<-ts(CO2_dataset, start = 1959, freq = 12)
plot(time_series)
# Chunk 3
'the mean should be 0 and the variance should be constant in every time to have a second order time series.
as soon as you see there is a trend= the mean is not=0---> is not a time series. the randomness could be second order stationary time serious.'
# Chunk 4
CO2_decomp <- decompose(time_series)
plot(CO2_decomp)
'you can see yearly means and data '
'trend=tendencia-average of each year and juntar puntos
observed=the sample in time with all he observations
seasonal=we can say that there is a repetitive pattern in the yearly CO2 amount as we can see it in the graph. so every year the CO2 repites but the mean says that this repeptitive pattern has a higher value.
RAndom=noise, error that cannot be controlled like high frequency signals in
'
CO2_trend <- CO2_decomp$trend
window(CO2_trend, end = c(1959, 12))
# Chunk 5
CO2_rand<-na.omit(CO2_decomp$random)
plot(CO2_rand)
Co2_acf <- acf(CO2_rand)
'is a periodic correlogram, there is a seasonalit going above the blue lines so it can be said that there is not a white noise. There is not white noise=if there is an interaction between the other values with the prevoius ones and the actual one. there is white noise if there is interaction just with the last value and the previous one to that vaue. The ACF compares the all the values in the observation and the pacf the actual value with the previous one.'
# Chunk 6
'the model AR1 is not a good model, we were expecting and exponential graph but there is bumping graph.'
n <- length(CO2_rand)
y <- CO2_rand[2:n]
x <- CO2_rand[1:(n-1)]
fitlm <- lm(y ~ x - 1)
'the lm function makes the linear redression of y and x ,you make the correlation between y and x-(2 vectors),  so you are comparing the last value in the vector, which is the actual one with all the previous ones(all of them).'
summary(fitlm)
'p value very low, so we can say that x (last value) and y (the previous value) are really strong correlated. '
# Chunk 7
' autocorrelation coefficient=alfa=which is looking to the table=0.40841
p value very low, so we can say that x (last value) and y (the previous value) are really strong correlated.'
'What is the estimated lag 1 autocorrelation coefficient=is the alfa
'
# Chunk 8
xn_1<-0.40841*CO2_rand
'to make two plots in'
# Chunk 9
xn_10<-0.40841*10*CO2_rand
# Chunk 10
residuals<-residuals(fitlm)
par(mfrow = c(2,1), mar = c(4,4,1,1))
'to compare two graphs'
plot(residuals)
acf(residuals)
'From the plot we obtained we could have said thath it is white noise, but from the correlogrm it is hown that there is some seasonality with the legs at 5,11,12,17.'
# Chunk 11
'From the plot we obtained we could have said thath it is white noise, but from the correlogrm it is hown that there is some seasonality with the legs at 5,11,12,17.'
'It is not a good fit=there is not white noise, because if it had been white noise it should be good fitted. so the actual value is influenced by the other prvious ones
you do not '
# Chunk 12
# Chunk 13
n <- length(CO2_rand)
y2 <- CO2_rand[3:n]
x2<- CO2_rand[2:(n-1)]
x3<- CO2_rand[1:(n-2)]
fitlm2 <- lm(y2 ~ x2+x3 - 1)
summary(fitlm2)
'x2 is not significant, less than 5%'
# Chunk 14
'The partial autocorrelation is the function thath shows if there is any correlation between values that are not consecutive, the function is:'
# Chunk 15
'where wt isthe white noise and are the parameters to be estimated'
# Chunk 16
ar1<-arima(CO2_rand, order=c(1,0,0))
ar2<-arima(CO2_rand, order=c(2,0,0))
ma1<-arima(CO2_rand, order=c(0,0,1))
# Chunk 17
# Chunk 1: setup
knitr::opts_chunk$set(eval = TRUE)
# Chunk 2
par(mfrow = c(2,1), mar = c(1,1,1,1))
plot(co2)
CO2_month <- ts(co2, start = 1959, freq = 12)
CO2year <- aggregate(CO2_month, FUN = mean)
plot(CO2year)
# Chunk 3
cyc <- cycle(CO2_month)
cyc <- factor(cyc, labels = month.abb)
boxplot(CO2_month ~ cyc)
# Chunk 4
CO2_decomp <- decompose(CO2_month)
CO2_trend <- CO2_decomp$trend
CO2_no_trend <- CO2_month - CO2_trend
plot(CO2_decomp)
# Chunk 5
ts.plot(CO2_trend,CO2_no_trend+CO2_trend, lty = c(1,2))
# Chunk 6
par(mfrow = c(2,1), mar = c(1,1,1,1))
plot(CO2_decomp$random)
co2rand <- na.omit(CO2_decomp$random)
acf(co2rand)
# Chunk 7
n <- length(co2rand)
y <- co2rand[2:n]
x <- co2rand[1:(n-1)]
fitlm <- lm(y ~ x - 1)
summary(fitlm)
# Chunk 8
alpha <- coef(fitlm)
x_n = co2rand[n]
x_n10 = alpha^10*x_n
x_n10
# Chunk 9
par(mfrow = c(2,1), mar = c(1,1,1,1))
fit_residuals <- residuals(fitlm)
plot(fit_residuals)
acf(fit_residuals)
# Chunk 10
n <- length(co2rand)
x <- co2rand[3:n]
x1 <- co2rand[2:(n-1)]
x2 <- co2rand[1:(n-2)]
fitlm2 <- lm(x ~ x1 + x2 - 1) # -1 is for removing the intercept.
summary(fitlm2)
# Chunk 11
ar1 <- arima(co2rand, order = c(1,0,0))
ar2 <- arima(co2rand, order = c(2,0,0))
ma1 <- arima(co2rand, order = c(0,0,1))
ma2 <- arima(co2rand, order = c(0,0,2))
arma11 <- arima(co2rand, order = c(1,0,1))
arma12 <- arima(co2rand, order = c(1,0,2))
arma21 <- arima(co2rand, order = c(2,0,1))
arma22 <- arima(co2rand, order = c(2,0,2))
AIC(ar1,ar2,ma1,ma2,arma11,arma12,arma21,arma22)
# Chunk 12
arma22
# Chunk 13
par(mfrow = c(2,1), mar = c(1,1,1,1))
resid_arma22 <- na.omit(arma22$residuals)
plot(resid_arma22)
acf(resid_arma22)
# Chunk 14
confint(arma22)
# Chunk 15
pred1 <- predict(arma22, n.ahead = 24)
lower1 <- pred1$pred-2*pred1$se
upper1 <- pred1$pred+2*pred1$se
ts.plot(co2rand, pred1$pred, lower1, upper1, lty = c(1,2,3,3), xlim=c(1996,1999))
# Chunk 16
pred1 <- predict(arma22, n.ahead = 1)
as.numeric(pred1$pred) + 2*as.numeric(pred1$se) * c(-1 ,1)
# Chunk 17
x_bar <- mean(co2rand)
se <- var(co2rand)
n <- length(co2rand)
lim <- sqrt(se*(1+1/n))
x_bar + c(-lim, lim)
par(mfrow = c(2,1), mar = c(1,1,1,1))
plot(co2)
CO2_month <- ts(co2, start = 1959, freq = 12)
CO2year <- aggregate(CO2_month, FUN = mean)
plot(CO2year)
n <- length(co2rand)
n
y <- co2rand[2:n]
x <- co2rand[1:(n-1)]
fitlm <- lm(y ~ x - 1)
summary(fitlm)
n <- length(co2rand)
n
y <- co2rand[2:n]
y
x <- co2rand[1:(n-1)]
x
fitlm <- lm(y ~ x - 1)
summary(fitlm)
alpha <- coef(fitlm)
x_n = co2rand[n]
x_n
x_n10 = alpha^10*x_n
x_n10
# Chunk 1: setup
knitr::opts_chunk$set(eval = TRUE)
# Chunk 2
par(mfrow = c(2,1), mar = c(2,2,2,2))
plot(co2)
CO2_month <- ts(co2, start = 1959, freq = 12)
CO2year <- aggregate(CO2_month, FUN = mean)
plot(CO2year)
# Chunk 3
cyc <- cycle(CO2_month)
cyc <- factor(cyc, labels = month.abb)
boxplot(CO2_month ~ cyc)
abline(h=mean(co2), col = "red", lwd = 2)
# Chunk 4
CO2_decomp <- decompose(CO2_month)
CO2_trend <- CO2_decomp$trend
CO2_no_trend <- CO2_month - CO2_trend
plot(CO2_decomp)
# Chunk 5
par(mfrow = c(2,1), mar = c(3,3,3,3))
plot(CO2_decomp$random)
co2rand <- na.omit(CO2_decomp$random)
acf(co2rand)
# Chunk 6
n <- length(co2rand)
y <- co2rand[2:n]
x <- co2rand[1:(n-1)]
fitlm <- lm(y ~ x-1)
summary(fitlm)
# Chunk 7
alpha <- coef(fitlm)
x_n = co2rand[n]
x_n10 = alpha^10*x_n
x_n10
# Chunk 8
par(mfrow = c(2,1), mar = c(3,3,3,3))
fit_residuals <- residuals(fitlm)
plot(fit_residuals)
acf(fit_residuals)
# Chunk 9
n <- length(co2rand)
x <- co2rand[3:n]
x1 <- co2rand[2:(n-1)]
x2 <- co2rand[1:(n-2)]
fitlm2 <- lm(x ~ x1 + x2 - 1) # -1 is for removing the intercept.
summary(fitlm2)
# Chunk 10
ar1 <- arima(co2rand, order = c(1,0,0))
ar2 <- arima(co2rand, order = c(2,0,0))
ma1 <- arima(co2rand, order = c(0,0,1))
ma2 <- arima(co2rand, order = c(0,0,2))
arma11 <- arima(co2rand, order = c(1,0,1))
arma12 <- arima(co2rand, order = c(1,0,2))
arma21 <- arima(co2rand, order = c(2,0,1))
arma22 <- arima(co2rand, order = c(2,0,2))
AIC(ar1,ar2,ma1,ma2,arma11,arma12,arma21,arma22)
# Chunk 11
arma22
# Chunk 12
par(mfrow = c(2,1), mar = c(1,1,1,1))
resid_arma22 <- na.omit(arma22$residuals)
plot(resid_arma22)
acf(resid_arma22)
# Chunk 13
confint(arma22)
# Chunk 14
pred1 <- predict(arma22, n.ahead = 24)
lower1 <- pred1$pred-2*pred1$se
upper1 <- pred1$pred+2*pred1$se
ts.plot(co2rand, pred1$pred, lower1, upper1, lty = c(1,2,3,3), xlim=c(1996,1999))
# Chunk 15
pred1 <- predict(arma22, n.ahead = 1)
as.numeric(pred1$pred) + 2*as.numeric(pred1$se) * c(-1 ,1)
# Chunk 16
x_bar <- mean(co2rand)
se <- var(co2rand)
n <- length(co2rand)
lim <- sqrt(se*(1+1/n))
x_bar + c(-lim, lim)
y1
y2
t = (y1-y2)/sed
t
y1
y2
t = (y1-y2)/sed
t
